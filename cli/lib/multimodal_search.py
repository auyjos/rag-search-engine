"""Multimodal search using CLIP embeddings for text and images."""

from PIL import Image
from sentence_transformers import SentenceTransformer


class MultimodalSearch:
    """
    Multimodal search class that can generate embeddings for both text and images.
    Uses CLIP models to create embeddings in the same vector space.
    """
    
    def __init__(self, model_name="clip-ViT-B-32"):
        """
        Initialize the multimodal search with a CLIP model.
        
        Args:
            model_name: Name of the CLIP model to use (default: "clip-ViT-B-32")
        """
        self.model = SentenceTransformer(model_name)
    
    def embed_image(self, image_path: str):
        """
        Generate an embedding for an image.
        
        Args:
            image_path: Path to the image file
            
        Returns:
            Image embedding as a numpy array
        """
        # Load the image
        image = Image.open(image_path)
        
        # Generate embedding (encode returns a list, we take the first element)
        embedding = self.model.encode([image])[0]
        
        return embedding


def verify_image_embedding(image_path: str):
    """
    Verify that image embeddings can be generated by creating an instance,
    generating an embedding, and printing its shape.
    
    Args:
        image_path: Path to the image file
    """
    # Create MultimodalSearch instance
    multimodal_search = MultimodalSearch()
    
    # Generate embedding
    embedding = multimodal_search.embed_image(image_path)
    
    # Print shape
    print(f"Embedding shape: {embedding.shape[0]} dimensions")